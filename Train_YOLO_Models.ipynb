{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdjeElectronics/Train-and-Deploy-YOLO-Models/blob/main/Train_YOLO_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is still a work in progress! - Evan Juras 2024-12-31\n"
      ],
      "metadata": {
        "id": "lWuf5vcyX-0G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sUfcA8ZgR2t"
      },
      "source": [
        "# Train YOLO Models in Google Colab\n",
        "**Author:** Evan Juras, [EJ Technology Consultants](https://ejtech.io)\n",
        "\n",
        "**Last updated:** December 31, 2024\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This notebook uses [Ultralytics](https://docs.ultralytics.com/) to train YOLO11, YOLOv8, or YOLOv5 object detection models with a custom dataset. At the end of this Colab, you'll have a custom YOLO model that you can run on your PC, phone, or edge device like the Raspberry Pi.\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://s3.us-west-1.amazonaws.com/evanjuras.com/img/yolo-model-demo.gif\" height=\"360\"><br>\n",
        "<i>Custom YOLO candy detection model in action!</i>\n",
        "</p>\n",
        "\n",
        "I created a YouTube video that walks through this guide step by step.I recommend following along with the video while working through this notebook.\n",
        "\n",
        "*Youtube video will be added when it's ready!*\n",
        "\n",
        "**Important note: This notebook will be continuously updated to make sure it works with newer versions of Ultralytics and YOLO. If you see any differences between the YouTube video and this notebook, always follow the notebook!**\n",
        "\n",
        "### Working in Colab\n",
        "Colab provides a virtual machine in your browser complete with a Linux OS, filesystem, Python environment, and best of all, a free GPU. We'll install PyTorch and Ultralytics in this environment and use it to train our model. Simply click the Play button on sections of code in this notebook to execute them on the virtual machine.\n",
        "\n",
        "### Navigation\n",
        "To navigate this notebook, use the table of contents in the left sidebar to jump from section to section.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verify NVIDIA GPU Availability**\n",
        "\n",
        "Make sure you're using a GPU-equipped machine by going to \"Runtime\" -> \"Change runtime type\" in the top menu bar, and then selecting one of the GPU options in the Hardware accelerator section. Click Play on the following code block to verify that the NVIDIA GPU is present and ready for training."
      ],
      "metadata": {
        "id": "3NW7LLv_QPOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "cfaWho47RGDf",
        "outputId": "0a1fdf88-22c9-4394-f2d4-10481fed41c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 31 19:49:54 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIHu25pnjjJ1"
      },
      "source": [
        "#1.&nbsp;Gather and Label Training Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6Y1vBiRjpcq"
      },
      "source": [
        "Before we start training, we need to gather and label images that will be used for training the object detection model. A good starting point for a proof-of-concept model is 200 images. The training images should have random objects in the image along with the desired objects, and should have a variety of backgrounds and lighting conditions.\n",
        "\n",
        "There are a couple options for gathering images:\n",
        "\n",
        "\n",
        "*   Build a custom dataset by taking your own pictures of the objects and labeling them (this typically results in the best performance)\n",
        "*   Find a pre-made dataset from sources like [Roboflow Universe](), [Kaggle](), or [Google Images V7]()\n",
        "\n",
        "\n",
        "If you want to build your own dataset, there are several tools available for labeling images. One good option is [Label Studio](https://labelstud.io/?utm_source=youtube&utm_medium=video&utm_campaign=edjeelectronics), a free and open-source labeling tool that has a simple workflow while providing capabilities for more advanced features. My YouTube video that walks through this notebook (link to be added soon) shows how to label images with Label Studio.\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/label-studio-example.PNG\" height=\"380\"><br>\n",
        "<i>Example of a candy image labeled with Label Studio.</i>\n",
        "</p>\n",
        "\n",
        "If you used Label Studio to label and export the images, they'll be exported in a `project.zip` file that contains the following:\n",
        "\n",
        "- An `images` folder containing the images\n",
        "- A `labels` folder containing the labels in YOLO annotation format\n",
        "- A `classes.txt` labelmap file that contains all the classes\n",
        "- A `notes.json` file that contains info specific to Label Studio (this file can be ignored)\n",
        "\n",
        "If you obtained your dataset from another source (like Roboflow Universe) or used another tool to label your dataset, make sure the files are organized in the same folder structure.\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/zipped-data-example.png\" height=\"\"><br>\n",
        "<i>Organize your data in the folders shown here. See my <a href=\"(https://www.dropbox.com/scl/fi/9x0wby2cbxhrwd8er65ox/YOLO_coin_data_12DEC30.zip?dl=0)\">Candy Detection Dataset</a> for an example.</i>\n",
        "</p>\n",
        "\n",
        "Once you've got your dataset built, put into the file structure shown above, and zipped into `data.zip`, you're ready to move on to the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eDhuvzDfIFS"
      },
      "source": [
        "# 2. Upload Image Dataset and Prepare Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW_0c110fOiz"
      },
      "source": [
        "Next, we'll upload our dataset and prepare it for training with YOLO. We'll split the dataset into train and validation folders, and we'll automatically generate the configuration file for training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Upload images\n",
        "\n",
        "First, we need to upload the dataset to Colab. Here are a few options for moving the `data.zip` folder into this Colab instance."
      ],
      "metadata": {
        "id": "FwKAqFIQSBpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Option 1. Upload through Google Colab**\n",
        "\n",
        "Upload the `data.zip` file to the Google Colab instance by clicking the \"Files\" icon on the left hand side of the browser, and then the \"Upload to session storage\" icon. Select the zip folder to upload it.\n",
        "\n",
        "<p>\n",
        "<br>\n",
        "<img src=\"https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/upload-colab-files.png\" height=\"240\">\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "ZPZEM27IOh79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Option 2. Copy from Google Drive**\n",
        "You can also upload your images to your personal Google Drive, mount the drive on this Colab session, and copy them over to the Colab filesystem. This option works well if you want to upload the images beforehand so you don't have to wait for them to upload each time you restart this Colab. If you have more than 50MB worth of images, I recommend using this option.\n",
        "\n",
        "First, upload the `data.zip` file to your Google Drive, and make note of the folder you uploaded them to. Replace `MyDrive/path/to/data.zip` with the path to your zip file. (For example, I uploaded the zip file to folder called \"candy-dataset1\", so I would use `MyDrive/candy-dataset1/data.zip` for the path). Then, run the following block of code to mount your Google Drive to this Colab session and copy the folder to this filesystem."
      ],
      "metadata": {
        "id": "TC4bZM1UWRdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp /content/gdrive/MyDrive/path/to/data.zip /content"
      ],
      "metadata": {
        "id": "ZfQBSwDdWoWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Option 3. Use my candy detection dataset**\n",
        "\n",
        "If you just want to test the process on a pre-made dataset, you can use my candy image dataset, which contains 160 pictures of popular candies (Skittles, Snickers, etc). Download it by running the following code block."
      ],
      "metadata": {
        "id": "q43_b9-sWsdB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQXLBvL5grDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85bd9f7f-fb49-4636-9e6d-9915711f4b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-31 03:30:49--  https://s3.us-west-1.amazonaws.com/evanjuras.com/resources/candy_data_14DEC24.zip\n",
            "Resolving s3.us-west-1.amazonaws.com (s3.us-west-1.amazonaws.com)... 52.219.216.0, 52.219.117.152, 52.219.121.80, ...\n",
            "Connecting to s3.us-west-1.amazonaws.com (s3.us-west-1.amazonaws.com)|52.219.216.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 236626389 (226M) [application/zip]\n",
            "Saving to: ‘/content/data.zip’\n",
            "\n",
            "/content/data.zip   100%[===================>] 225.66M  99.4MB/s    in 2.3s    \n",
            "\n",
            "2024-12-31 03:30:51 (99.4 MB/s) - ‘/content/data.zip’ saved [236626389/236626389]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# To use my pre-made candy dataset instead of your own custom dataset, download it here\n",
        "!wget -O /content/data.zip https://s3.us-west-1.amazonaws.com/evanjuras.com/resources/candy_data_14DEC24.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Create directories and split images into train and validation folders"
      ],
      "metadata": {
        "id": "m7Iz9eBzW5zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, whether you used Option 1, 2, or 3, you should be able to click the folder icon on the left and see your `data.zip` file in the list of files. Next, we'll unzip `data.zip` and create some folders to hold the images. Run the following two code blocks to unzip the data and create folders for holding the images and labels."
      ],
      "metadata": {
        "id": "58JuFGc2PatU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8O6z-wVcPEF"
      },
      "outputs": [],
      "source": [
        "# Unzip images to a custom data folder\n",
        "!unzip -q /content/data.zip -d /content/custom_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2A_RcSq89y6"
      },
      "outputs": [],
      "source": [
        "# Make ALL the directories!! This sets up the specific directory structure needed by YOLO for training models\n",
        "# TO DO: Add file checking in case the zipped folder already has train and validation folders (i.e. if they got it from Roboflow Universe)\n",
        "!mkdir -p /content/datasets/my_dataset/train/images; mkdir /content/datasets/my_dataset/train/labels\n",
        "!mkdir -p /content/datasets/my_dataset/validation/images; mkdir /content/datasets/my_dataset/validation/labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll split the images into train and validation sets:\n",
        "\n",
        "*   **Train**: These are the actual images used to train the model. In one epoch of training, every image in the train set is passed into the neural network. The training algorithm adjusts the network weights to fit the data in the images.\n",
        "\n",
        "\n",
        "*   **Validation**: These images are used to check the model's performance at the end of each training epoch.\n",
        "\n",
        "I wrote a Python script to randomly move 90% of images to the \"train\" folder and 10% to the \"validation\" folders. Run the following code block to execute the scrpt."
      ],
      "metadata": {
        "id": "eoPjqW6AYebn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNkzgI8H_cZC"
      },
      "outputs": [],
      "source": [
        "# Split between train and val folders\n",
        "\n",
        "from pathlib import Path\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Set train and val split amounts\n",
        "train_percent = 0.9  # 90% of the files go to train\n",
        "val_percent = 0.1 # 10% go to validation\n",
        "\n",
        "# Define paths to image and annotation folders\n",
        "image_path = '/content/custom_data/images'\n",
        "label_path = '/content/custom_data/labels'\n",
        "\n",
        "# In case dataset got uploaded with a root \"data\" folder:\n",
        "if os.path.is\n",
        "train_img_path = '/content/datasets/my_dataset/train/images'\n",
        "train_txt_path = '/content/datasets/my_dataset/train/labels'\n",
        "val_img_path = '/content/datasets/my_dataset/validation/images'\n",
        "val_txt_path = '/content/datasets/my_dataset/validation/labels'\n",
        "\n",
        "# Get list of all images and annotation files\n",
        "img_file_list = [path for path in Path(image_path).rglob('*')]\n",
        "txt_file_list = [path for path in Path(label_path).rglob('*')]\n",
        "\n",
        "print(f'Number of image files: {len(img_file_list)}')\n",
        "print(f'Number of annotation files: {len(txt_file_list)}')\n",
        "\n",
        "# Determine number of files to move to each folder\n",
        "file_num = len(img_file_list)\n",
        "train_num = int(file_num*train_percent)\n",
        "val_num = file_num - train_num\n",
        "print('Images moving to train: %d' % train_num)\n",
        "print('Images moving to validation: %d' % val_num)\n",
        "\n",
        "# Select files randomly and move them to train or val folders\n",
        "for i, set_num in enumerate([train_num, val_num]):\n",
        "  for ii in range(set_num):\n",
        "    img_path = random.choice(img_file_list)\n",
        "    img_fn = img_path.name\n",
        "    base_fn = img_path.stem\n",
        "    txt_fn = base_fn + '.txt'\n",
        "    txt_path = os.path.join(label_path,txt_fn)\n",
        "\n",
        "    if i == 0: # Move first set of files to train folders\n",
        "      new_img_path, new_txt_path = train_img_path, train_txt_path\n",
        "    elif i == 1: # Move second set of files to the validation folders\n",
        "      new_img_path, new_txt_path = val_img_path, val_txt_path\n",
        "\n",
        "    os.rename(img_path, os.path.join(new_img_path,img_fn))\n",
        "    if os.path.exists(txt_path): # If txt path does not exist, this is a background image, so skip txt file\n",
        "      os.rename(txt_path,os.path.join(new_txt_path,txt_fn))\n",
        "\n",
        "    img_file_list.remove(img_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python hey.py --datapath=\"/content/custom_data\" --train_pct=0.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrih1S6sxG3Q",
        "outputId": "7ea11bdc-8240-47f9-fbaa-99eb91b1816d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created folder at /content/data/train/images.\n",
            "Created folder at /content/data/train/labels.\n",
            "Created folder at /content/data/validation/images.\n",
            "Created folder at /content/data/validation/labels.\n",
            "Number of image files: 162\n",
            "Number of annotation files: 162\n",
            "Images moving to train: 129\n",
            "Images moving to validation: 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy-x7_MlKKU5"
      },
      "outputs": [],
      "source": [
        "# Create objects.names file (not sure which step this should be at)\n",
        "### This creates a \"objects.names\" file with a list of classes the object detection model will detect.\n",
        "%%bash\n",
        "cat <<EOF >> /content/objects.names\n",
        "penny\n",
        "nickel\n",
        "dime\n",
        "quarter\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Install Requirements (Ultralytics)\n",
        "\n",
        "Next, we'll install the Ultralytics library in this Google Colab instance. This Python library will be used to train the YOLO model."
      ],
      "metadata": {
        "id": "B2L2qGCJzwY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "EMEDk5byzxY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuZoMkSFN9XG"
      },
      "source": [
        "# 4. Configure Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyM9JxA6OANK"
      },
      "outputs": [],
      "source": [
        "# Create the .yaml file that holds training config data\n",
        "# Unfortunately, there's not an easy file to copy from. I think we should just create it with the cat command, like before. Or with Python.\n",
        "# TODO: do it programmatically by storing a list of classes as a python variable or something\n",
        "\n",
        "%%bash\n",
        "cat <<EOF >> /content/data.yaml\n",
        "path: /content/datasets/my_dataset\n",
        "train: train/images\n",
        "val: validation/images\n",
        "\n",
        "nc: 11\n",
        "\n",
        "names: [\"MMs_peanut\", \"MMs_regular\", \"airheads\", \"gummy_worms\", \"milky_way\", \"nerds\", \"skittles\", \"snickers\", \"starburst\", \"three_musketeers\", \"twizzlers\"]\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myP80_bnTNMi"
      },
      "source": [
        "# 5. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bbpob1gTPlo"
      },
      "outputs": [],
      "source": [
        "!yolo detect train data=/content/data.yaml model=yolo11s.pt epochs=40 imgsz=640"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo8BJRXeg0Ap"
      },
      "source": [
        "#6. Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PooP5Vjsg2Jn"
      },
      "outputs": [],
      "source": [
        "!yolo detect predict model=runs/detect/train/weights/best.pt source=datasets/my_dataset/validation/images save=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEEObQqoiGrs"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "for image_path in glob.glob(f'/content/runs/detect/predict/*.jpg')[:10]:\n",
        "  display(Image(filename=image_path, height=400))\n",
        "  print('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7yrFRViVczX"
      },
      "source": [
        "#7. Deploy Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, zip and download the trained model by running the code blocks below."
      ],
      "metadata": {
        "id": "IcoBAeHXa86W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create \"my_model\" folder to store model weights and train results\n",
        "!mkdir /content/my_model\n",
        "!cp /content/runs/detect/train/weights/best.pt /content/my_model/my_model.pt\n",
        "!cp -r /content/runs/detect/train /content/my_model\n",
        "\n",
        "# Zip into \"my_model.zip\"\n",
        "%cd my_model\n",
        "!zip /content/my_model.zip my_model.pt\n",
        "!zip -r /content/my_model.zip train\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "qcBdnOA9v85S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ypwonynLVu"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/my_model.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy Model"
      ],
      "metadata": {
        "id": "ure0gCW4ynH0"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNM5a+rS57B4HswAQ9spfHu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}